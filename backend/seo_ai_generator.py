#!/usr/bin/env python3
"""
SEO AI Files Generator
Analyzes SEO report JSON and generates optimized robots.txt and llms.txt files
using OpenAI GPT-4 for intelligent, context-aware recommendations.

All content is dynamically generated by AI based on the actual SEO analysis -
no hardcoded templates or values.

Usage:
    python seo_ai_generator.py repot.json
    python seo_ai_generator.py repot.json --output-dir ./output
    python seo_ai_generator.py repot.json --dry-run
"""

import asyncio
import json
import os
import sys
import argparse
from datetime import datetime
from urllib.parse import urlparse
from typing import Optional

# Add parent directory to path for config import
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

try:
    from config import get_api_key, GENERATED_DIR_STR
except ImportError:
    # Fallback if config not available
    GENERATED_DIR_STR = "./generated"
    def get_api_key(provider):
        return os.environ.get(f"{provider.upper()}_API_KEY")

try:
    import aiohttp
except ImportError:
    print("Missing aiohttp. Install with: pip install aiohttp")
    sys.exit(1)


# ============================================================================
# AI Bot Reference (for AI context, not hardcoded rules)
# ============================================================================

AI_BOT_REFERENCE = """
Known AI/LLM Crawlers and their purposes:
- GPTBot: OpenAI training & ChatGPT search
- OAI-SearchBot: ChatGPT web search feature
- ChatGPT-User: ChatGPT user browsing
- ClaudeBot: Anthropic Claude training
- Claude-Web: Claude web access
- anthropic-ai: Anthropic AI systems
- Google-Extended: Google Gemini/Bard
- GoogleOther: Google AI services
- PerplexityBot: Perplexity AI search
- Applebot-Extended: Apple AI/Siri
- CCBot: Common Crawl (AI training data)
- Amazonbot: Amazon Alexa/AI
- cohere-ai: Cohere AI
- Diffbot: Diffbot knowledge graph
- Meta-ExternalAgent: Meta AI
- FacebookBot: Meta/Facebook AI
- Bytespider: ByteDance/TikTok (often aggressive)
- omgili: Webz.io (high volume scraper)
- SemrushBot: SEMrush (SEO tool)
- AhrefsBot: Ahrefs (SEO tool)
"""


# ============================================================================
# OpenAI Client
# ============================================================================

class OpenAIClient:
    """Async OpenAI client for generating dynamic content."""
    
    def __init__(self):
        self.api_key = get_api_key("openai")
        self.api_base = "https://api.openai.com/v1"
        
    def is_configured(self) -> bool:
        return bool(self.api_key)
    
    async def chat(self, messages: list, model: str = "gpt-4o", temperature: float = 0.7) -> dict:
        """Send chat completion request."""
        if not self.is_configured():
            return {"error": "OPENAI_API_KEY not set"}
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        
        payload = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": 4096,
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.api_base}/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=120)
                ) as resp:
                    if resp.status != 200:
                        error = await resp.text()
                        return {"error": f"HTTP {resp.status}: {error[:200]}"}
                    
                    data = await resp.json()
                    return {
                        "content": data["choices"][0]["message"]["content"],
                        "tokens": data.get("usage", {}).get("total_tokens", 0)
                    }
        except Exception as e:
            return {"error": f"Request failed: {str(e)}"}


# ============================================================================
# AI-Powered Robots.txt Generator
# ============================================================================

class RobotsTxtGenerator:
    """Generate AI-optimized robots.txt based on SEO report analysis."""
    
    def __init__(self, report: dict, openai_client: OpenAIClient):
        self.report = report
        self.openai = openai_client
        self.url = report.get("url", "")
        self.parsed_url = urlparse(self.url)
        self.base_url = f"{self.parsed_url.scheme}://{self.parsed_url.netloc}"
        
    async def generate(self) -> str:
        """Generate robots.txt content using AI analysis."""
        if not self.openai.is_configured():
            print("OpenAI not configured, using minimal fallback", file=sys.stderr)
            return self._minimal_fallback()
        
        # Prepare context for AI
        report_summary = self._prepare_report_summary()
        
        prompt = f"""You are an SEO expert. Generate a complete, production-ready robots.txt file for this website based on the SEO analysis data.

WEBSITE ANALYSIS:
{report_summary}

KNOWN AI CRAWLERS REFERENCE:
{AI_BOT_REFERENCE}

REQUIREMENTS:
1. Analyze the website's structure, content type, and purpose from the SEO data
2. Determine which paths should be allowed/disallowed based on the actual site structure
3. Set appropriate crawl-delay values based on site size and server response times
4. Configure AI bot permissions intelligently:
   - Allow high-value AI crawlers (GPTBot, ClaudeBot, etc.) for content-rich sites
   - Consider blocking aggressive scrapers if site has performance concerns
5. Include sitemap reference if detected
6. Add relevant comments explaining the rationale for each section

OUTPUT FORMAT:
Return ONLY the robots.txt content, no explanations or markdown code blocks.
Include comments (lines starting with #) to explain each section.
Make sure the file is well-organized and follows robots.txt best practices.

Generate the robots.txt now:"""

        messages = [
            {"role": "system", "content": "You are an expert SEO consultant specializing in robots.txt optimization and AI crawler management. Generate production-ready robots.txt files based on website analysis."},
            {"role": "user", "content": prompt}
        ]
        
        print("Generating AI-powered robots.txt...", file=sys.stderr)
        result = await self.openai.chat(messages, temperature=0.3)
        
        if "error" in result:
            print(f"AI generation failed: {result['error']}, using fallback", file=sys.stderr)
            return self._minimal_fallback()
        
        content = result.get("content", "").strip()
        
        # Clean up any markdown code blocks if AI included them
        if content.startswith("```"):
            lines = content.split("\n")
            content = "\n".join(lines[1:-1] if lines[-1].strip() == "```" else lines[1:])
        
        return content
    
    def _prepare_report_summary(self) -> str:
        """Prepare a summary of the SEO report for AI context."""
        metadata = self.report.get("metadata", {})
        content = self.report.get("content", {})
        ai_indexing = self.report.get("ai_indexing", {})
        headings = self.report.get("headings", {})
        links = self.report.get("links", {})
        technical = self.report.get("technical", {})
        
        # Get existing robots.txt status
        robots_status = ai_indexing.get("robots_txt", {})
        ai_bot_status = ai_indexing.get("ai_bot_status", {})
        
        # Get sitemaps
        sitemaps = robots_status.get("sitemaps_declared", [])
        sitemap_url = sitemaps[0] if sitemaps else f"{self.base_url}/sitemap.xml"
        
        summary = f"""
URL: {self.url}
Domain: {self.parsed_url.netloc}
Base URL: {self.base_url}

SITE METADATA:
- Title: {metadata.get('title', {}).get('value', 'N/A')}
- Description: {metadata.get('description', {}).get('value', 'N/A')}
- Language: {metadata.get('language', 'en')}
- Has Canonical: {bool(metadata.get('canonical'))}

CONTENT ANALYSIS:
- Word Count: {content.get('word_count', 0)}
- Top Keywords: {[k.get('keyword') for k in content.get('keywords_tfidf', [])[:10]]}
- Readability Grade: {content.get('readability', {}).get('flesch_kincaid_grade', 'N/A')}

SITE STRUCTURE:
- H1 Count: {headings.get('h1', {}).get('count', 0)}
- H2 Count: {headings.get('h2', {}).get('count', 0)}
- H2 Sections: {headings.get('h2', {}).get('values', [])[:5]}
- Internal Links: {links.get('internal_count', 0)}
- External Links: {links.get('external_count', 0)}

TECHNICAL:
- Response Time: {technical.get('response_time_ms', 0)}ms
- Has SSL: {self.url.startswith('https')}

EXISTING ROBOTS.TXT STATUS:
- Exists: {robots_status.get('exists', False)}
- Current Sitemaps: {sitemaps}

CURRENT AI BOT STATUS (from existing robots.txt):
{json.dumps(ai_bot_status, indent=2) if ai_bot_status else 'No existing AI bot rules detected'}

RECOMMENDED SITEMAP URL: {sitemap_url}
"""
        return summary
    
    def _minimal_fallback(self) -> str:
        """Minimal fallback if AI is not available."""
        sitemaps = self.report.get("ai_indexing", {}).get("robots_txt", {}).get("sitemaps_declared", [])
        sitemap_url = sitemaps[0] if sitemaps else f"{self.base_url}/sitemap.xml"
        
        return f"""# robots.txt for {self.parsed_url.netloc}
# Generated: {datetime.now().strftime('%Y-%m-%d')}

User-agent: *
Allow: /

Sitemap: {sitemap_url}
"""


# ============================================================================
# AI-Powered LLMs.txt Generator  
# ============================================================================

class LlmsTxtGenerator:
    """Generate AI-powered llms.txt based on SEO report analysis."""
    
    def __init__(self, report: dict, openai_client: OpenAIClient):
        self.report = report
        self.openai = openai_client
        self.url = report.get("url", "")
        self.parsed_url = urlparse(self.url)
        self.base_url = f"{self.parsed_url.scheme}://{self.parsed_url.netloc}"
        
    async def generate(self) -> str:
        """Generate llms.txt content using AI analysis."""
        if not self.openai.is_configured():
            print("OpenAI not configured, using minimal fallback", file=sys.stderr)
            return self._minimal_fallback()
        
        # Prepare comprehensive context for AI
        report_context = self._prepare_full_context()
        
        prompt = f"""You are an expert at creating llms.txt files following the llmstxt.org specification. Generate a comprehensive, production-ready llms.txt file for this website.

WEBSITE ANALYSIS DATA:
{report_context}

LLMS.TXT SPECIFICATION:
- llms.txt is a markdown file that helps AI/LLM systems understand and interact with a website
- It should include: site overview, what the site does, key content areas, permissions, and technical details
- Use markdown formatting (headers, lists, code blocks, etc.)
- Be factual and based on the actual analysis data provided

REQUIREMENTS:
1. Create a comprehensive site overview based on the actual content analysis
2. List the main topics/expertise areas based on detected keywords
3. Describe the site structure based on headings and navigation
4. Include appropriate AI/LLM permissions based on the site type
5. Add technical details for AI agents (crawl rates, content types, etc.)
6. If structured data (JSON-LD, Open Graph) was detected, mention it
7. Make all content specific to THIS website - no generic placeholders

OUTPUT FORMAT:
Return ONLY the llms.txt content in valid markdown format.
No explanations before or after, no code block markers wrapping the whole output.
Start directly with the markdown content (typically starting with # Site Name).

Generate the llms.txt now:"""

        messages = [
            {"role": "system", "content": "You are an expert at creating llms.txt files for websites. You analyze SEO reports and generate comprehensive, accurate llms.txt content that helps AI systems understand websites. Always base your content on the actual data provided, never use placeholder or generic content."},
            {"role": "user", "content": prompt}
        ]
        
        print("Generating AI-powered llms.txt...", file=sys.stderr)
        result = await self.openai.chat(messages, temperature=0.5)
        
        if "error" in result:
            print(f"AI generation failed: {result['error']}, using fallback", file=sys.stderr)
            return self._minimal_fallback()
        
        content = result.get("content", "").strip()
        
        # Clean up any markdown code blocks if AI wrapped the output
        if content.startswith("```markdown"):
            lines = content.split("\n")
            content = "\n".join(lines[1:-1] if lines[-1].strip() == "```" else lines[1:])
        elif content.startswith("```"):
            lines = content.split("\n")
            content = "\n".join(lines[1:-1] if lines[-1].strip() == "```" else lines[1:])
        
        return content
    
    def _prepare_full_context(self) -> str:
        """Prepare comprehensive context from SEO report for AI."""
        metadata = self.report.get("metadata", {})
        content = self.report.get("content", {})
        structured = self.report.get("structured_data", {})
        headings = self.report.get("headings", {})
        links = self.report.get("links", {})
        ai_indexing = self.report.get("ai_indexing", {})
        scores = self.report.get("scores", {})
        issues = self.report.get("issues", [])
        
        # Extract keywords
        keywords_tfidf = [k.get("keyword") for k in content.get("keywords_tfidf", [])[:15]]
        keywords_freq = [k.get("keyword") for k in content.get("keywords_frequency", [])[:15]]
        bigrams = [p.get("phrase") for p in content.get("top_bigrams", [])[:10]]
        trigrams = [p.get("phrase") for p in content.get("top_trigrams", [])[:10]]
        
        # Get Open Graph data
        og = structured.get("open_graph", {})
        twitter = structured.get("twitter_card", {})
        json_ld = structured.get("json_ld", [])
        
        # Get headings structure
        h1_values = headings.get("h1", {}).get("values", [])
        h2_values = headings.get("h2", {}).get("values", [])
        h3_values = headings.get("h3", {}).get("values", [])
        
        context = f"""
URL: {self.url}
Domain: {self.parsed_url.netloc}
Base URL: {self.base_url}
Analysis Date: {self.report.get('timestamp', datetime.now().isoformat())}

=== METADATA ===
Title: {metadata.get('title', {}).get('value', 'N/A')}
Description: {metadata.get('description', {}).get('value', 'N/A')}
Language: {metadata.get('language', 'en')}
Keywords Meta: {metadata.get('keywords_meta', 'N/A')}
Canonical URL: {metadata.get('canonical', 'N/A')}

=== CONTENT ANALYSIS ===
Word Count: {content.get('word_count', 0)}
Readability:
- Flesch Reading Ease: {content.get('readability', {}).get('flesch_reading_ease', 'N/A')}
- Grade Level: {content.get('readability', {}).get('flesch_kincaid_grade', 'N/A')}
- Reading Time: {content.get('readability', {}).get('reading_time_minutes', 'N/A')} minutes

=== TOP KEYWORDS (TF-IDF Analysis) ===
{json.dumps(keywords_tfidf, indent=2)}

=== TOP KEYWORDS (Frequency) ===
{json.dumps(keywords_freq, indent=2)}

=== KEY PHRASES (Bigrams) ===
{json.dumps(bigrams, indent=2)}

=== KEY PHRASES (Trigrams) ===
{json.dumps(trigrams, indent=2)}

=== SITE STRUCTURE (Headings) ===
H1 Headings: {json.dumps(h1_values, indent=2)}
H2 Headings (Sections): {json.dumps(h2_values[:10], indent=2)}
H3 Headings (Subsections): {json.dumps(h3_values[:10], indent=2)}

=== LINKS ===
Internal Links: {links.get('internal_count', 0)}
External Links: {links.get('external_count', 0)}
Nofollow Links: {links.get('nofollow_count', 0)}

=== STRUCTURED DATA ===
Open Graph:
{json.dumps(og, indent=2) if og else 'Not detected'}

Twitter Card:
{json.dumps(twitter, indent=2) if twitter else 'Not detected'}

JSON-LD Schema Types: {[item.get('@type') for item in json_ld] if json_ld else 'Not detected'}

=== SEO SCORES ===
{json.dumps(scores, indent=2) if scores else 'N/A'}

=== AI INDEXING STATUS ===
Has robots.txt: {ai_indexing.get('robots_txt', {}).get('exists', False)}
Has llms.txt: {ai_indexing.get('llms_txt', {}).get('exists', False)}
Has sitemap: {ai_indexing.get('sitemap', {}).get('exists', False)}
Sitemaps Declared: {ai_indexing.get('robots_txt', {}).get('sitemaps_declared', [])}

=== KEY ISSUES DETECTED ===
{json.dumps([{'severity': i.get('severity'), 'message': i.get('message')} for i in issues[:10]], indent=2) if issues else 'No major issues'}
"""
        return context
    
    def _minimal_fallback(self) -> str:
        """Minimal fallback if AI is not available."""
        metadata = self.report.get("metadata", {})
        og = self.report.get("structured_data", {}).get("open_graph", {})
        
        title = metadata.get("title", {}).get("value", self.parsed_url.netloc)
        description = metadata.get("description", {}).get("value", "")
        site_name = og.get("site_name", self.parsed_url.netloc)
        
        return f"""# {site_name}

> {description if description else 'Website'}

## Overview

- **URL:** {self.base_url}
- **Language:** {metadata.get('language', 'en')}

## AI/LLM Permissions

This site allows AI crawlers to index public content.

---

Generated: {datetime.now().strftime('%Y-%m-%d')}
"""


# ============================================================================
# Main Generator with Full AI Integration
# ============================================================================

class SEOAIGenerator:
    """Main class to generate AI-optimized SEO files using ChatGPT."""
    
    def __init__(self, report_path: str, output_dir: str = "."):
        self.report_path = report_path
        self.output_dir = output_dir
        self.openai = OpenAIClient()
        self.report = None
        
    def load_report(self) -> bool:
        """Load SEO report JSON."""
        try:
            with open(self.report_path, 'r') as f:
                self.report = json.load(f)
            return True
        except Exception as e:
            print(f"Error loading report: {e}", file=sys.stderr)
            return False
    
    async def generate_all(self, dry_run: bool = False) -> dict:
        """Generate all files using AI."""
        if not self.load_report():
            return {"error": "Failed to load report"}
        
        results = {
            "url": self.report.get("url"),
            "timestamp": datetime.now().isoformat(),
            "ai_powered": self.openai.is_configured(),
            "files_generated": [],
            "tokens_used": 0
        }
        
        if not self.openai.is_configured():
            print("WARNING: OPENAI_API_KEY not set. Files will use minimal fallback templates.", file=sys.stderr)
            print("Set OPENAI_API_KEY in .env for AI-powered generation.", file=sys.stderr)
        
        # Generate robots.txt using AI
        print("=" * 50, file=sys.stderr)
        print("Generating robots.txt...", file=sys.stderr)
        robots_gen = RobotsTxtGenerator(self.report, self.openai)
        robots_content = await robots_gen.generate()
        
        # Generate llms.txt using AI
        print("=" * 50, file=sys.stderr)
        print("Generating llms.txt...", file=sys.stderr)
        llms_gen = LlmsTxtGenerator(self.report, self.openai)
        llms_content = await llms_gen.generate()
        
        # Output files
        robots_path = os.path.join(self.output_dir, "robots.txt")
        llms_path = os.path.join(self.output_dir, "llms.txt")
        
        if dry_run:
            print("\n" + "="*60, file=sys.stderr)
            print("DRY RUN - robots.txt", file=sys.stderr)
            print("="*60, file=sys.stderr)
            print(robots_content)
            print("\n" + "="*60, file=sys.stderr)
            print("DRY RUN - llms.txt", file=sys.stderr)
            print("="*60, file=sys.stderr)
            print(llms_content)
        else:
            # Create output directory
            os.makedirs(self.output_dir, exist_ok=True)
            
            # Write robots.txt
            with open(robots_path, 'w') as f:
                f.write(robots_content)
            results["files_generated"].append(robots_path)
            print(f"✓ Generated: {robots_path}", file=sys.stderr)
            
            # Write llms.txt
            with open(llms_path, 'w') as f:
                f.write(llms_content)
            results["files_generated"].append(llms_path)
            print(f"✓ Generated: {llms_path}", file=sys.stderr)
        
        # Generate AI-powered meta tag recommendations
        results["meta_recommendations"] = await self._generate_meta_recommendations()
        
        print("=" * 50, file=sys.stderr)
        print(f"Generation complete. AI-powered: {results['ai_powered']}", file=sys.stderr)
        
        return results
    
    async def _generate_meta_recommendations(self) -> dict:
        """Generate AI-powered meta tag recommendations."""
        if not self.openai.is_configured():
            return self._fallback_meta_recommendations()
        
        metadata = self.report.get("metadata", {})
        structured = self.report.get("structured_data", {})
        content = self.report.get("content", {})
        
        prompt = f"""Based on this SEO analysis, generate specific meta tag recommendations for improved AI discoverability.

CURRENT META DATA:
- Title: {metadata.get('title', {}).get('value', 'N/A')}
- Description: {metadata.get('description', {}).get('value', 'N/A')}
- Has Open Graph: {bool(structured.get('open_graph'))}
- Has Twitter Card: {bool(structured.get('twitter_card'))}
- Has JSON-LD: {bool(structured.get('json_ld'))}
- Top Keywords: {[k.get('keyword') for k in content.get('keywords_tfidf', [])[:5]]}

Provide recommendations as a JSON object with:
1. "add_to_head": array of HTML meta tags to add (as strings)
2. "json_ld_recommendation": a recommended JSON-LD schema object

Return ONLY valid JSON, no explanations or markdown."""

        messages = [
            {"role": "system", "content": "You are an SEO expert. Return only valid JSON."},
            {"role": "user", "content": prompt}
        ]
        
        result = await self.openai.chat(messages, temperature=0.3)
        
        if "error" in result:
            return self._fallback_meta_recommendations()
        
        try:
            content = result.get("content", "").strip()
            # Clean up potential markdown code blocks
            if content.startswith("```json"):
                content = content[7:]
            if content.startswith("```"):
                content = content[3:]
            if content.endswith("```"):
                content = content[:-3]
            return json.loads(content.strip())
        except json.JSONDecodeError:
            return self._fallback_meta_recommendations()
    
    def _fallback_meta_recommendations(self) -> dict:
        """Fallback meta recommendations if AI is not available."""
        return {
            "add_to_head": [
                '<meta name="robots" content="index, follow">',
            ],
            "json_ld_recommendation": {
                "@context": "https://schema.org",
                "@type": "WebPage",
                "name": self.report.get("metadata", {}).get("title", {}).get("value", ""),
                "url": self.report.get("url", "")
            }
        }


# ============================================================================
# CLI Interface
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Generate AI-powered robots.txt and llms.txt from SEO report using ChatGPT",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python seo_ai_generator.py repot.json
  python seo_ai_generator.py repot.json --output-dir ./public
  python seo_ai_generator.py repot.json --dry-run

Environment:
  OPENAI_API_KEY - Required for AI-powered generation (set in .env file)
  
Note: All content is dynamically generated by ChatGPT based on the SEO analysis.
Without OPENAI_API_KEY, minimal fallback templates will be used.
        """
    )
    
    parser.add_argument("report", help="Path to SEO report JSON file")
    parser.add_argument("-o", "--output-dir", default=".", help="Output directory (default: current)")
    parser.add_argument("--dry-run", action="store_true", help="Print generated content without saving")
    parser.add_argument("-p", "--pretty", action="store_true", help="Pretty print JSON results")
    
    args = parser.parse_args()
    
    # Check for API key
    if not os.environ.get("OPENAI_API_KEY"):
        print("=" * 60, file=sys.stderr)
        print("WARNING: OPENAI_API_KEY not set!", file=sys.stderr)
        print("Files will use minimal fallback templates.", file=sys.stderr)
        print("Set OPENAI_API_KEY in .env for full AI-powered generation.", file=sys.stderr)
        print("=" * 60, file=sys.stderr)
    else:
        print("OPENAI_API_KEY detected. Using AI-powered generation.", file=sys.stderr)
    
    # Run generator
    generator = SEOAIGenerator(args.report, args.output_dir)
    result = asyncio.run(generator.generate_all(dry_run=args.dry_run))
    
    # Print results
    indent = 2 if args.pretty else None
    print(json.dumps(result, indent=indent))


if __name__ == "__main__":
    main()
